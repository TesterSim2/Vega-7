# ==============================================================================
#           PROJECT VEGA 7: A DYNAMIC RECURRENT MODEL
#
# This script implements the distillation of a large Transformer (Qwen2-4B)
# into a custom, advanced RNN architecture named "Vega 7".
#
# Architectural Concepts Implemented:
# 1. Finch/Goose: Matrix-valued states and dynamic, data-dependent recurrence.
# 2. GhostRNN: Splits state updates into expensive (active) and cheap (ghost)
#    operations to manage computational cost.
# 3. Latent Reasoning: The model "thinks" for multiple steps internally for
#    each input token, simulating a deeper model.
# ==============================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc # Garbage Collector for memory management

# ==============================================================================
# 1. HYPERPARAMETERS & CONFIGURATION
# ==============================================================================

# --- Teacher Model ---
TEACHER_MODEL_NAME = "Qwen/Qwen2-0.5B" # Using 0.5B for Colab compatibility. Change to Qwen/Qwen2-4B if you have high-RAM GPU.

# --- Student Model (Vega 7) ---
STUDENT_HIDDEN_SIZE = 512       # Must match teacher's hidden_size if inheriting. Can be different.
STUDENT_NUM_LAYERS = 8          # Number of LatentThinkingBlocks
ACTIVE_CHANNELS = 128           # (GhostRNN) Number of matrix rows for expensive updates.
NUM_THINKING_STEPS = 4          # (Latent Reasoning) Inner loop steps per token.

# --- Training ---
LEARNING_RATE = 1e-4
NUM_DISTILLATION_EPOCHS = 10    # More epochs for a more complex task
NUM_SFT_EPOCHS = 3
TEMPERATURE = 2.0               # Distillation temperature
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Using device: {DEVICE}")
if "cuda" in DEVICE:
    print(f"CUDA Device Name: {torch.cuda.get_device_name(0)}")

# ==============================================================================
# 2. VEGA 7 STUDENT MODEL DEFINITION
# ==============================================================================

class DynamicRecurrenceModule(nn.Module):
    """
    Implements the Finch/Goose + GhostRNN dynamic recurrence.
    It updates a matrix-valued state [H, H] based on an input vector x [H].
    """
    def __init__(self, hidden_size, active_channels):
        super().__init__()
        self.hidden_size = hidden_size
        self.active_channels = active_channels
        self.ghost_channels = hidden_size - active_channels

        # (Finch/Goose) Network to generate dynamic parameters from input x
        self.param_generator = nn.Linear(hidden_size, 2 * active_channels)
        
        # (GhostRNN) Cheap, shared linear transformation for the ghost part of the state
        self.ghost_update = nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, state, x):
        # state shape: [batch, hidden_size, hidden_size]
        # x shape: [batch, hidden_size]

        # Split state into active and ghost components
        active_state, ghost_state = torch.split(
            state, [self.active_channels, self.ghost_channels], dim=1
        )

        # 1. Expensive, Dynamic Update for Active State
        # Generate data-dependent parameters (u, v) from input x
        params = self.param_generator(x)
        u, v = torch.chunk(params, 2, dim=-1) # u, v shape: [batch, active_channels]
        
        # Apply a simple dynamic update. This can be made more complex.
        # Reshape u and v to broadcast correctly with the active_state matrix
        active_update = active_state * F.silu(u.unsqueeze(-1)) + v.unsqueeze(-1)

        # 2. Cheap, Shared Update for Ghost State
        ghost_update = self.ghost_update(ghost_state)

        # 3. Recombine and return the new state with a residual connection
        new_state = torch.cat([active_update, ghost_update], dim=1)
        new_state = state + new_state # Residual connection on the state
        return new_state

class LatentThinkingBlock(nn.Module):
    """
    A single layer of the Vega 7 model. Implements the latent reasoning loop.
    """
    def __init__(self, hidden_size, active_channels, num_thinking_steps):
        super().__init__()
        self.num_thinking_steps = num_thinking_steps
        self.recurrence = DynamicRecurrenceModule(hidden_size, active_channels)
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, 4 * hidden_size),
            nn.GELU(),
            nn.Linear(4 * hidden_size, hidden_size)
        )
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, state, x):
        # state shape: [batch, hidden_size, hidden_size]
        # x shape: [batch, seq_len, hidden_size]
        
        # 1. Inject input into state
        # We only use the last token's representation to update the state
        # In a real scenario, you'd loop over the sequence length
        state = self.recurrence(state, self.norm1(x[:, -1, :]))

        # 2. Latent Thinking Loop
        # The model "thinks" by refining its state without new external input
        if self.num_thinking_steps > 1:
            # Create a zero vector for non-input steps
            null_input = torch.zeros_like(x[:, -1, :])
            for _ in range(self.num_thinking_steps - 1):
                state = self.recurrence(state, null_input)

        # 3. Project state back to a vector for the feed-forward layer
        # A simple mean over one dimension of the matrix state
        state_vector = torch.mean(state, dim=1)

        # 4. Apply feed-forward layer with residual connection
        x = x + self.feed_forward(self.norm2(state_vector)).unsqueeze(1)
        return state, x

class Vega7Model(nn.Module):
    """
    The complete Vega 7 student model.
    """
    def __init__(self, vocab_size, hidden_size, num_layers, active_channels, num_thinking_steps):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.layers = nn.ModuleList([
            LatentThinkingBlock(hidden_size, active_channels, num_thinking_steps)
            for _ in range(num_layers)
        ])
        self.output_norm = nn.LayerNorm(hidden_size)
        self.output = nn.Linear(hidden_size, vocab_size)
        self.hidden_size = hidden_size
        self.num_layers = num_layers

    def forward(self, input_ids, states):
        batch_size = input_ids.shape[0]
        x = self.embedding(input_ids)

        new_states = []
        for i, layer in enumerate(self.layers):
            state, x = layer(states[i], x)
            new_states.append(state)

        x = self.output_norm(x)
        logits = self.output(x)
        return logits, new_states

# ==============================================================================
# 3. KNOWLEDGE DISTILLATION SETUP
# ==============================================================================

# --- Loss Function (Unchanged from original Colab) ---
class KnowledgeDistillationLoss(nn.Module):
    def __init__(self, temperature=1.0):
        super().__init__()
        self.temperature = temperature
        self.kl_div = nn.KLDivLoss(reduction="batchmean")

    def forward(self, student_logits, teacher_logits):
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)
        return self.kl_div(student_log_probs, teacher_probs)

# --- Load Teacher Model ---
print(f"Loading teacher model: {TEACHER_MODEL_NAME}...")
try:
    teacher_model = AutoModelForCausalLM.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)
    teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)
    # Update student hidden size to match teacher if not specified
    STUDENT_HIDDEN_SIZE = teacher_model.config.hidden_size
    print("Teacher model loaded successfully.")
except Exception as e:
    print(f"Error loading teacher model: {e}")
    print("This might be a RAM issue. For Colab, try using a smaller model like 'Qwen/Qwen2-0.5B' or upgrading your instance.")
    # Clean up and exit if model loading fails
    teacher_model = None
    gc.collect()
    torch.cuda.empty_cache()


if teacher_model:
    # --- Initialize Student Model (Vega 7) ---
    vocab_size = teacher_model.config.vocab_size
    student_model = Vega7Model(
        vocab_size=vocab_size,
        hidden_size=STUDENT_HIDDEN_SIZE,
        num_layers=STUDENT_NUM_LAYERS,
        active_channels=ACTIVE_CHANNELS,
        num_thinking_steps=NUM_THINKING_STEPS
    ).to(DEVICE)

    print("\n--- Model Architectures ---")
    print("Teacher Model:", teacher_model.config.model_type)
    print("Student Model: Vega 7")
    print(f"  - Hidden Size: {STUDENT_HIDDEN_SIZE}")
    print(f"  - Num Layers: {STUDENT_NUM_LAYERS}")
    print(f"  - Active/Ghost Channels: {ACTIVE_CHANNELS}/{STUDENT_HIDDEN_SIZE - ACTIVE_CHANNELS}")
    print(f"  - Latent Thinking Steps: {NUM_THINKING_STEPS}")
    
    # --- Training Setup ---
    optimizer = optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)
    distillation_loss_fn = KnowledgeDistillationLoss(temperature=TEMPERATURE)

    # --- Dummy Dataset for Distillation ---
    text = "The Qwen model, a large language model developed by Alibaba Cloud, has demonstrated exceptional performance on various benchmarks."
    inputs = teacher_tokenizer(text, return_tensors="pt").to(DEVICE)
    input_ids = inputs["input_ids"]

    # ==============================================================================
    # 4. DISTILLATION TRAINING LOOP
    # ==============================================================================
    print("\n--- Starting Knowledge Distillation ---")

    # Initialize Vega 7's matrix-valued states (one for each layer)
    batch_size = input_ids.shape[0]
    states = [
        torch.zeros(batch_size, STUDENT_HIDDEN_SIZE, STUDENT_HIDDEN_SIZE).to(DEVICE)
        for _ in range(STUDENT_NUM_LAYERS)
    ]

    for epoch in range(NUM_DISTILLATION_EPOCHS):
        student_model.train()
        teacher_model.eval()

        # Forward pass through teacher
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids, labels=input_ids)
            teacher_logits = teacher_outputs.logits

        # Forward pass through student
        student_logits, new_states = student_model(input_ids, states)

        # IMPORTANT: Detach states from the computation graph for the next iteration
        states = [s.detach() for s in new_states]

        # Compute loss
        loss = distillation_loss_fn(student_logits, teacher_logits)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0) # Gradient clipping
        optimizer.step()

        print(f"Epoch {epoch + 1}/{NUM_DISTILLATION_EPOCHS}, Loss: {loss.item():.6f}")

    print("--- Distillation Finished ---")

    # ==============================================================================
    # 5. SUPERVISED FINE-TUNING (SFT)
    # ==============================================================================

    def supervised_fine_tuning(model, tokenizer, dataset, num_epochs=3):
        print("\n--- Starting Supervised Fine-Tuning ---")
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE / 2) # Lower LR for fine-tuning
        loss_fn = nn.CrossEntropyLoss()

        for epoch in range(num_epochs):
            for i, batch in enumerate(dataset):
                input_ids = tokenizer(batch["text"], return_tensors="pt")["input_ids"].to(DEVICE)
                labels = input_ids.clone()
                
                # Initialize a fresh, zero state for each independent batch
                batch_size = input_ids.shape[0]
                sft_states = [
                    torch.zeros(batch_size, model.hidden_size, model.hidden_size).to(DEVICE)
                    for _ in range(model.num_layers)
                ]

                logits, _ = model(input_ids, sft_states)
                
                # Reshape for CrossEntropyLoss
                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                print(f"SFT Epoch {epoch + 1}, Batch {i+1}, Loss: {loss.item():.4f}")

    # Example SFT Dataset
    dummy_sft_dataset = [{"text": "Instruction: Write a poem about AI.\nOutput: In circuits of silicon, a mind begins to bloom."}]
    supervised_fine_tuning(student_model, teacher_tokenizer, dummy_sft_dataset, num_epochs=NUM_SFT_EPOCHS)
    
    # ==============================================================================
    # 6. SAVE THE FINAL MODEL
    # ==============================================================================
    
    FINAL_MODEL_PATH = "vega7_distilled_model.pt"
    torch.save(student_model.state_dict(), FINAL_MODEL_PATH)
    print(f"\n--- Model Saved ---")
    print(f"Final Vega 7 model state dictionary saved to: {FINAL_MODEL_PATH}")

    # Clean up GPU memory
    del teacher_model
    del student_model
    gc.collect()
    torch.cuda.empty_cache()
